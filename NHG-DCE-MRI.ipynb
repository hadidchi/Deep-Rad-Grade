{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T07:12:09.504975Z",
     "iopub.status.busy": "2025-01-23T07:12:09.504695Z",
     "iopub.status.idle": "2025-01-23T07:12:18.914034Z",
     "shell.execute_reply": "2025-01-23T07:12:18.913384Z",
     "shell.execute_reply.started": "2025-01-23T07:12:09.504947Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)  \n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "import seaborn as sns\n",
    "import re\n",
    "import os\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':16:8'\n",
    "import glob\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pydicom\n",
    "from scipy.ndimage import zoom\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy import stats\n",
    "from scipy.stats import ttest_ind\n",
    "from fastai.vision.all import *\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter\n",
    "from IPython.display import Image\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "import torch.optim as optim\n",
    "import torch.cuda.amp as amp\n",
    "from torchmetrics.classification import BinaryAccuracy\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "from sklearn.utils import resample\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "from IPython.display import display, clear_output\n",
    "import nibabel as nib\n",
    "from skimage.transform import resize\n",
    "from torchsummary import summary\n",
    "from torchvision import models\n",
    "from torchvision.models.resnet import ResNet, BasicBlock\n",
    "from lifelines import CoxPHFitter, KaplanMeierFitter\n",
    "from lifelines.utils import concordance_index\n",
    "from lifelines.statistics import multivariate_logrank_test, logrank_test\n",
    "from lifelines.plotting import add_at_risk_counts\n",
    "import torchio as tio\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Pre-Processing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T07:12:18.915827Z",
     "iopub.status.busy": "2025-01-23T07:12:18.915067Z",
     "iopub.status.idle": "2025-01-23T07:12:18.920740Z",
     "shell.execute_reply": "2025-01-23T07:12:18.919929Z",
     "shell.execute_reply.started": "2025-01-23T07:12:18.915785Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    \"\"\"An abstract class representing a Dataset.\n",
    "\n",
    "    All other datasets should subclass it. All subclasses should override\n",
    "    ``__len__``, that provides the size of the dataset, and ``__getitem__``,\n",
    "    supporting integer indexing in range from 0 to len(self) exclusive.\n",
    "    \"\"\"\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return ConcatDataset([self, other])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T07:12:18.922683Z",
     "iopub.status.busy": "2025-01-23T07:12:18.922364Z",
     "iopub.status.idle": "2025-01-23T07:12:18.946912Z",
     "shell.execute_reply": "2025-01-23T07:12:18.946117Z",
     "shell.execute_reply.started": "2025-01-23T07:12:18.922647Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MRI(Dataset):\n",
    "    def __init__(self, rd, dfs, sf, test=0.2):\n",
    "        self.mode = 'train'\n",
    "\n",
    "        # Labels\n",
    "        rd_label = np.ones(rd.shape[0], dtype=np.float32)\n",
    "        dfs_label = np.zeros(dfs.shape[0], dtype=np.float32)\n",
    "\n",
    "        # Concatenate images and labels\n",
    "        self.labels = np.concatenate((rd_label, dfs_label))\n",
    "\n",
    "        # Combine image arrays but don't load them into memory until needed\n",
    "        self.rd = rd\n",
    "        self.dfs = dfs\n",
    "        self.image_indices = np.concatenate((np.arange(rd.shape[0]), np.arange(dfs.shape[0])))\n",
    "\n",
    "        # Split the data into train and validation sets (indices only)\n",
    "        self.train_val_split(sf, test)\n",
    "\n",
    "    def train_val_split(self, sf, test):\n",
    "        # Split only indices to avoid duplicating data in memory\n",
    "        self.train_idx, self.val_idx = train_test_split(np.arange(len(self.labels)), test_size=test, random_state=sf, shuffle=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.mode == 'train':\n",
    "            return len(self.train_idx)\n",
    "        else:\n",
    "            return len(self.val_idx)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Select the correct index based on the mode\n",
    "        if self.mode == 'train':\n",
    "            idx = self.train_idx[idx]\n",
    "        else:\n",
    "            idx = self.val_idx[idx]\n",
    "\n",
    "        # Determine whether the image comes from rd or dfs\n",
    "        if idx < len(self.rd):\n",
    "            image = self.rd[idx]\n",
    "        else:\n",
    "            image = self.dfs[idx - len(self.rd)]\n",
    "\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return {'image': image, 'label': label}\n",
    "\n",
    "    def set_mode(self, mode):\n",
    "        if mode in ['train', 'val']:\n",
    "            self.mode = mode\n",
    "        else:\n",
    "            raise ValueError(\"Mode should be 'train' or 'val'\")\n",
    "\n",
    "    def get_train_rd_count(self):\n",
    "        \"\"\"Returns the number of rd samples in the training set.\"\"\"\n",
    "        rd_indices = [idx for idx in self.train_idx if idx < len(self.rd)]\n",
    "        return len(rd_indices)\n",
    "\n",
    "    def get_train_dfs_count(self):\n",
    "        \"\"\"Returns the number of dfs samples in the training set.\"\"\"\n",
    "        dfs_indices = [idx for idx in self.train_idx if idx >= len(self.rd)]\n",
    "        return len(dfs_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Table 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T07:12:18.948900Z",
     "iopub.status.busy": "2025-01-23T07:12:18.948652Z",
     "iopub.status.idle": "2025-01-23T07:12:18.962501Z",
     "shell.execute_reply": "2025-01-23T07:12:18.961671Z",
     "shell.execute_reply.started": "2025-01-23T07:12:18.948877Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def perform_chi_square(observed, expected, total_observed, total_expected):\n",
    "    if total_observed == 0 or total_expected == 0:\n",
    "        return \"\"\n",
    "    \n",
    "    observed_frequencies = [observed, total_observed - observed]\n",
    "    expected_frequencies = [expected, total_expected - expected]\n",
    "    \n",
    "    # Check for zeros in the expected frequencies\n",
    "    if 0 in expected_frequencies:\n",
    "        return \"\"\n",
    "    \n",
    "    contingency_table = [observed_frequencies, expected_frequencies]\n",
    "    chi2, p_value, _, _ = chi2_contingency(contingency_table)\n",
    "    \n",
    "    if p_value < 0.001:\n",
    "        return \"***\"\n",
    "    elif p_value < 0.01:\n",
    "        return \"**\"\n",
    "    elif p_value < 0.05:\n",
    "        return \"*\"\n",
    "    else:\n",
    "        return \"\"\n",
    "        \n",
    "def format_output(count, total_in_interval, baseline_count, total_baseline):\n",
    "    percentage = 100 * count / total_in_interval if total_in_interval != 0 else 0\n",
    "    star = perform_chi_square(count, baseline_count, total_in_interval, total_baseline)\n",
    "    return f\"{int(count)} ({percentage:.2f}%) {star}\"\n",
    "\n",
    "def summarize_data(group, baseline_group, total_baseline, cat_columns_dict, cont_columns_dict):\n",
    "    total_in_interval = len(group)  # Total number of cases in this interval\n",
    "\n",
    "    summary = {\n",
    "        'Number of Patients, n': total_in_interval,\n",
    "    }\n",
    "\n",
    "    # Summarize continuous variables with t-test\n",
    "    for display_name, (col_name, stat_method) in cont_columns_dict.items():\n",
    "        # Perform t-test between the group and baseline group\n",
    "        t_stat, p_value = ttest_ind(group[col_name], baseline_group[col_name], nan_policy='omit', equal_var=False)\n",
    "\n",
    "        # Determine significance level\n",
    "        if p_value < 0.001:\n",
    "            star = \"***\"\n",
    "        elif p_value < 0.01:\n",
    "            star = \"**\"\n",
    "        elif p_value < 0.05:\n",
    "            star = \"*\"\n",
    "        else:\n",
    "            star = \"\"\n",
    "\n",
    "        # Calculate and format statistics based on preferred method\n",
    "        if stat_method == 'mean_std':\n",
    "            mean = group[col_name].mean()\n",
    "            std = group[col_name].std()\n",
    "            summary[display_name] = f\"{mean:.2f} Â± {std:.2f} {star}\"\n",
    "        elif stat_method == 'median_iqr':\n",
    "            median = group[col_name].median()\n",
    "            q1 = group[col_name].quantile(0.25)\n",
    "            q3 = group[col_name].quantile(0.75)\n",
    "            summary[display_name] = f\"{median:.2f} [{q1:.2f}, {q3:.2f}] {star}\"\n",
    "        else:\n",
    "            # Default to mean (SD) if stat_method is not recognized\n",
    "            mean = group[col_name].mean()\n",
    "            std = group[col_name].std()\n",
    "            summary[display_name] = f\"{mean:.2f} ({std:.2f}) {star}\"\n",
    "\n",
    "    # Summarize categorical variables with chi-square test\n",
    "    for display_name, col_name in cat_columns_dict.items():\n",
    "        if col_name == \"\":\n",
    "            summary[display_name] = \"\"\n",
    "        else:\n",
    "            count = group[col_name].sum()\n",
    "            baseline_count = baseline_group[col_name].sum()\n",
    "            summary[display_name] = format_output(count, total_in_interval, baseline_count, total_baseline)\n",
    "\n",
    "    return pd.Series(summary)\n",
    "\n",
    "def generate_summary_table(df, cat_columns_dict, cont_columns_dict, baseline_interval=1, variable='COVID'):\n",
    "    df[variable] = pd.Categorical(df[variable])\n",
    "\n",
    "    baseline_group = df[df[variable] == baseline_interval]\n",
    "    total_baseline = len(baseline_group)\n",
    "\n",
    "    table = df.groupby(variable).apply(\n",
    "        lambda group: summarize_data(group, baseline_group, total_baseline, cat_columns_dict, cont_columns_dict)\n",
    "    )\n",
    "\n",
    "    table = table.transpose()\n",
    "\n",
    "    # Optional: Reorder columns to have the baseline group first\n",
    "    if baseline_interval in table.columns:\n",
    "        cols = [baseline_interval] + [col for col in table.columns if col != baseline_interval]\n",
    "        table = table[cols]\n",
    "\n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T07:12:34.714455Z",
     "iopub.status.busy": "2025-01-23T07:12:34.713793Z",
     "iopub.status.idle": "2025-01-23T07:12:34.718667Z",
     "shell.execute_reply": "2025-01-23T07:12:34.717786Z",
     "shell.execute_reply.started": "2025-01-23T07:12:34.714422Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def count_dcm_files(folder_path):\n",
    "    # Use glob to find all .dcm files in the folder\n",
    "    dcm_files = glob.glob(os.path.join(folder_path, '*.dcm'))\n",
    "\n",
    "    # Count the number of files found\n",
    "    num_dcm_files = len(dcm_files)\n",
    "\n",
    "    return num_dcm_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T07:12:35.526194Z",
     "iopub.status.busy": "2025-01-23T07:12:35.525921Z",
     "iopub.status.idle": "2025-01-23T07:12:35.530908Z",
     "shell.execute_reply": "2025-01-23T07:12:35.529999Z",
     "shell.execute_reply.started": "2025-01-23T07:12:35.526167Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def find_folders_with_dcm_files(root_folder):\n",
    "    folders_with_dcm = []\n",
    "\n",
    "    for folder_path, _, _ in os.walk(root_folder):\n",
    "        # Use glob to check for .dcm files in the current folder\n",
    "        dcm_files = glob.glob(os.path.join(folder_path, '*.dcm'))\n",
    "\n",
    "        if dcm_files:\n",
    "            # Add the full path of the folder to the list\n",
    "            folders_with_dcm.append(folder_path)\n",
    "\n",
    "    return folders_with_dcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T07:12:35.672694Z",
     "iopub.status.busy": "2025-01-23T07:12:35.672107Z",
     "iopub.status.idle": "2025-01-23T07:12:35.676850Z",
     "shell.execute_reply": "2025-01-23T07:12:35.675924Z",
     "shell.execute_reply.started": "2025-01-23T07:12:35.672665Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def sort_by_number_after_last_slash(lst):\n",
    "    # Sort the list by extracting the number after the last '/'\n",
    "    sorted_list = sorted(lst, key=lambda x: float(x.split('/')[-1].split('.')[0]))  # Extract number before first period\n",
    "    return sorted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T07:12:35.823434Z",
     "iopub.status.busy": "2025-01-23T07:12:35.822976Z",
     "iopub.status.idle": "2025-01-23T07:12:35.828076Z",
     "shell.execute_reply": "2025-01-23T07:12:35.827287Z",
     "shell.execute_reply.started": "2025-01-23T07:12:35.823408Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def display_first_file_name(folder_path):\n",
    "    try:\n",
    "        # List all files in the folder\n",
    "        files = os.listdir(folder_path)\n",
    "        # Filter out directories, only keep files\n",
    "        files = [f for f in files if os.path.isfile(os.path.join(folder_path, f))]\n",
    "\n",
    "        if files:\n",
    "            # Get the first file name\n",
    "            first_file = files[0]\n",
    "            return first_file\n",
    "        else:\n",
    "            return \"The folder is empty or contains no files.\"\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        return \"The specified folder does not exist.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T07:12:35.972629Z",
     "iopub.status.busy": "2025-01-23T07:12:35.972415Z",
     "iopub.status.idle": "2025-01-23T07:12:35.976527Z",
     "shell.execute_reply": "2025-01-23T07:12:35.975723Z",
     "shell.execute_reply.started": "2025-01-23T07:12:35.972607Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def count_digits_after_dash(filename):\n",
    "    # Use regex to find the pattern after the dash\n",
    "    match = re.search(r'-(\\d+)', filename)\n",
    "\n",
    "    if match:\n",
    "        # Return the length of the matched digits\n",
    "        return len(match.group(1))\n",
    "    else:\n",
    "        # Return 0 if no match is found\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T07:12:36.179937Z",
     "iopub.status.busy": "2025-01-23T07:12:36.179672Z",
     "iopub.status.idle": "2025-01-23T07:12:36.184896Z",
     "shell.execute_reply": "2025-01-23T07:12:36.184029Z",
     "shell.execute_reply.started": "2025-01-23T07:12:36.179913Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def resample_slices_gpu(image_np, target_slices):\n",
    "    # Convert numpy array to torch tensor and move to GPU\n",
    "    image_tensor = torch.tensor(image_np).cuda()\n",
    "    original_slices = image_tensor.shape[1]  # Number of slices in the original image\n",
    "\n",
    "    if original_slices != target_slices:\n",
    "        zoom_factor = target_slices / original_slices\n",
    "        # Adjust the tensor shape from (N, S, H, W) to (N, C, S, H, W) by adding a channel dimension\n",
    "        image_tensor = image_tensor.unsqueeze(1)  # (N, S, H, W) to (N, 1, S, H, W)\n",
    "        # Use interpolate to resample the slices\n",
    "        image_tensor = F.interpolate(image_tensor, size=(target_slices, image_tensor.shape[3], image_tensor.shape[4]), mode='trilinear', align_corners=False)\n",
    "        # Remove the added channel dimension\n",
    "        image_tensor = image_tensor.squeeze(1)  # (N, 1, S, H, W) to (N, S, H, W)\n",
    "\n",
    "    return image_tensor.cpu().numpy()  # Move back to CPU and convert to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T07:12:39.673047Z",
     "iopub.status.busy": "2025-01-23T07:12:39.672226Z",
     "iopub.status.idle": "2025-01-23T07:12:39.679152Z",
     "shell.execute_reply": "2025-01-23T07:12:39.678123Z",
     "shell.execute_reply.started": "2025-01-23T07:12:39.673014Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def pad_to_shape(image_np, target_shape, device='cpu'):\n",
    "    if len(image_np.shape) != 4 or len(target_shape) != 4:\n",
    "        raise ValueError(\"Both input tensor and target shape must be 4D.\")\n",
    "    \n",
    "    image_tensor = torch.tensor(image_np.copy(), device=device)\n",
    "    \n",
    "    # Calculate padding only for necessary dimensions\n",
    "    padding = []\n",
    "    for dim in range(4):\n",
    "        current_size = image_tensor.shape[dim]\n",
    "        target_size = target_shape[dim]\n",
    "        if current_size < target_size:\n",
    "            total_padding = target_size - current_size\n",
    "            pad_before = total_padding // 2\n",
    "            pad_after = total_padding - pad_before\n",
    "            padding.append((pad_before, pad_after))\n",
    "        else:\n",
    "            padding.append((0, 0))\n",
    "    \n",
    "    padding = [p for pair in reversed(padding) for p in pair]\n",
    "    padded_tensor = torch.nn.functional.pad(image_tensor, padding, mode='constant', value=0)\n",
    "    \n",
    "    return padded_tensor.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T07:12:39.867939Z",
     "iopub.status.busy": "2025-01-23T07:12:39.867713Z",
     "iopub.status.idle": "2025-01-23T07:12:39.872213Z",
     "shell.execute_reply": "2025-01-23T07:12:39.871400Z",
     "shell.execute_reply.started": "2025-01-23T07:12:39.867917Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def resize_image(image_np, target_shape):\n",
    "\n",
    "    # Unpacking target dimensions\n",
    "    target_channels, target_slices, target_height, target_width = target_shape\n",
    "    \n",
    "    # Creating resized image\n",
    "    resized_image = np.array([\n",
    "        [resize(slice, (target_height, target_width), anti_aliasing=True) \n",
    "         for slice in channel] for channel in image_np\n",
    "    ])\n",
    "    \n",
    "    return resized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T07:12:40.053806Z",
     "iopub.status.busy": "2025-01-23T07:12:40.053584Z",
     "iopub.status.idle": "2025-01-23T07:12:40.058191Z",
     "shell.execute_reply": "2025-01-23T07:12:40.057347Z",
     "shell.execute_reply.started": "2025-01-23T07:12:40.053784Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def resize_array_scipy(image_np, target_size):\n",
    "    \"\"\"\n",
    "    Resize a 5D NumPy array from (N, C, D, H, W) to (N, C, D, target_H, target_W).\n",
    "\n",
    "    Parameters:\n",
    "        image_np (numpy.ndarray): Input array with shape (N, C, D, H, W).\n",
    "        target_size (tuple): Tuple (target_H, target_W).\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Resized array with shape (N, C, D, target_H, target_W).\n",
    "    \"\"\"\n",
    "    N, C, D, H, W = image_np.shape\n",
    "    target_H, target_W = target_size\n",
    "\n",
    "    # Calculatea zoom factors\n",
    "    zoom_factors = [1, 1, 1, target_H / H, target_W / W]\n",
    "\n",
    "    # Perform the zoom\n",
    "    resized_array = zoom(image_np, zoom_factors, order=1)  # order=1 for bilinear interpolation\n",
    "\n",
    "    return resized_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T07:12:40.185119Z",
     "iopub.status.busy": "2025-01-23T07:12:40.184536Z",
     "iopub.status.idle": "2025-01-23T07:12:40.189088Z",
     "shell.execute_reply": "2025-01-23T07:12:40.188265Z",
     "shell.execute_reply.started": "2025-01-23T07:12:40.185094Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def find_midpoint(coord1, coord2):\n",
    "\n",
    "    if len(coord1) != 3 or len(coord2) != 3:\n",
    "        raise ValueError(\"Both coordinates must be lists or tuples of length 3.\")\n",
    "    \n",
    "    midpoint = [(c1 + c2) / 2 for c1, c2 in zip(coord1, coord2)]\n",
    "    return midpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T07:12:40.369733Z",
     "iopub.status.busy": "2025-01-23T07:12:40.369495Z",
     "iopub.status.idle": "2025-01-23T07:12:40.373687Z",
     "shell.execute_reply": "2025-01-23T07:12:40.372864Z",
     "shell.execute_reply.started": "2025-01-23T07:12:40.369710Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def sort_by_number_after_slash(lst):\n",
    "    # Sort the list using a custom key that extracts the number after the last '/'\n",
    "    sorted_list = sorted(lst, key=lambda x: int(x.split('-')[-1]))\n",
    "    return sorted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T07:12:40.511759Z",
     "iopub.status.busy": "2025-01-23T07:12:40.511199Z",
     "iopub.status.idle": "2025-01-23T07:12:40.522754Z",
     "shell.execute_reply": "2025-01-23T07:12:40.521845Z",
     "shell.execute_reply.started": "2025-01-23T07:12:40.511733Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def dcm_to_np_duke(patients, target_shape, device, segmentations):\n",
    "    patients_list = []\n",
    "    list_of_people_scans = []\n",
    "\n",
    "    # Iterate over each patient\n",
    "    for patient_path in patients:\n",
    "\n",
    "        # Get the DCE MRI loaded in as a np array\n",
    "        phase_paths = find_folders_with_dcm_files(patient_path)\n",
    "        sorted_phase_paths = sort_by_number_after_last_slash(phase_paths)\n",
    "        for offset in range(len(sorted_phase_paths)):\n",
    "            if 'dyn' in sorted_phase_paths[offset].split('/')[-1].lower() or 'vibrant' in sorted_phase_paths[offset].split('/')[-1].lower():\n",
    "                break\n",
    "        selected_phase_paths = sorted_phase_paths[offset:target_shape[0]+offset]\n",
    "        dcms = get_dicom_file_paths(selected_phase_paths)\n",
    "        print(int(patient_path[-3:]))\n",
    "        entire = np.stack([pydicom.dcmread(dcm).pixel_array for dcm in dcms], axis=0)     \n",
    "        entire = np.reshape(entire, (target_shape[0], entire.shape[0] // target_shape[0], entire.shape[1], entire.shape[2]))     \n",
    "\n",
    "        # Normalize\n",
    "        entire = (entire - np.min(entire)) / (np.max(entire) - np.min(entire))\n",
    "\n",
    "        # Grab sagittal tumor slices\n",
    "        start = segmentations[segmentations['Patient ID'] == patient_path[-14:]]['Start Sagittal'].values[0] -1\n",
    "        end = segmentations[segmentations['Patient ID'] == patient_path[-14:]]['End Sagittal'].values[0] - 1\n",
    "        middle = math.floor((start + end) // 2)\n",
    "        slices = target_shape[1]\n",
    "        start_at = max(0, int(middle - (slices // 2)))\n",
    "        end_at = min(entire.shape[3], int(start_at + slices))    \n",
    "        entire = entire[:, :, :, start_at:end_at]\n",
    "\n",
    "        # Grab coronal tumor slices\n",
    "        start = segmentations[segmentations['Patient ID'] == patient_path[-14:]]['Start Coronal'].values[0] -1\n",
    "        end = segmentations[segmentations['Patient ID'] == patient_path[-14:]]['End Coronal'].values[0] - 1\n",
    "        middle = math.floor((start + end) // 2)\n",
    "        slices = target_shape[3]\n",
    "        start_at = max(0, int(middle - (slices // 2)))\n",
    "        end_at = min(entire.shape[2], int(start_at + slices))    \n",
    "        entire = entire[:, :, start_at:end_at, :]\n",
    "\n",
    "        # Grab axial tumor slices\n",
    "        start = segmentations[segmentations['Patient ID'] == patient_path[-14:]]['Start Axial'].values[0] -1\n",
    "        end = segmentations[segmentations['Patient ID'] == patient_path[-14:]]['End Axial'].values[0] - 1\n",
    "        middle = math.floor((start + end) // 2)\n",
    "        slices = target_shape[2]\n",
    "        start_at = max(0, int(middle - (slices // 2)))\n",
    "        end_at = min(entire.shape[1], int(start_at + slices))    \n",
    "        entire = entire[:, start_at:end_at, :, :]\n",
    "\n",
    "        # Pad to shape if needed and add to large list\n",
    "        list_of_people_scans.append(pad_to_shape(entire, target_shape, device))\n",
    "\n",
    "        # Output Patinet ID\n",
    "        patients_list.append(int(patient_path[-3:]))\n",
    "\n",
    "    # Create output np array\n",
    "    output_np = np.stack(list_of_people_scans, axis=0).astype(np.float32)\n",
    "\n",
    "    return output_np, patients_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T07:12:40.698809Z",
     "iopub.status.busy": "2025-01-23T07:12:40.698021Z",
     "iopub.status.idle": "2025-01-23T07:12:40.703112Z",
     "shell.execute_reply": "2025-01-23T07:12:40.702407Z",
     "shell.execute_reply.started": "2025-01-23T07:12:40.698780Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_dummies(df, columns, prefix=True):\n",
    "    for column in columns:\n",
    "        # Replace null values with a placeholder\n",
    "        df[column] = df[column].fillna('Missing')\n",
    "        \n",
    "        # Create dummy variables\n",
    "        dummies = pd.get_dummies(df[column], prefix=column if prefix else None)\n",
    "        \n",
    "        # Convert dummies to integers\n",
    "        dummies = dummies.astype(int)\n",
    "        \n",
    "        # Concatenate the dummies to the original DataFrame\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T07:12:45.928039Z",
     "iopub.status.busy": "2025-01-23T07:12:45.927818Z",
     "iopub.status.idle": "2025-01-23T07:12:45.935258Z",
     "shell.execute_reply": "2025-01-23T07:12:45.934340Z",
     "shell.execute_reply.started": "2025-01-23T07:12:45.928018Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_slice(image_np, slice_index, size=(24,9), normalize_entire=True, cmap='gray', title=\"MRI Slice\"):\n",
    "    if len(image_np.shape) == 3:\n",
    "        image_np = np.expand_dims(image_np, axis=0)\n",
    "    clear_output(wait=True)  # Clear previous plots\n",
    "    plt.figure(figsize=size)\n",
    "    num_slices = image_np.shape[0]\n",
    "\n",
    "\n",
    "    for i in range(num_slices):\n",
    "        plt.subplot(1, num_slices, i + 1)\n",
    "        plt.imshow(image_np[i, slice_index, :, :], cmap=cmap, vmin=np.min(image_np if normalize_entire else image_np[i]), vmax=np.max(image_np if normalize_entire else image_np[i]))\n",
    "        plt.title(f'Phase {i+1}')\n",
    "        plt.axis('off')\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    display(slice_slider)  # Redisplay the slider\n",
    "    \n",
    "    def scroll(image_np, normalize_entire=True):\n",
    "        if len(image_np.shape) == 3:\n",
    "            image_np = np.expand_dims(image_np, axis=0)\n",
    "        max_slice_index = image_np.shape[1] - 1\n",
    "        global slice_slider\n",
    "        slice_slider = widgets.IntSlider(min=0, max=max_slice_index, step=1, value=0)\n",
    "\n",
    "        def on_value_change(change):\n",
    "            plot_slice(image_np, change['new'], normalize_entire=normalize_entire)\n",
    "\n",
    "        slice_slider.observe(on_value_change, names='value')\n",
    "        display(slice_slider)  # Initial display of the slider\n",
    "        plot_slice(image_np, slice_slider.value, normalize_entire=normalize_entire)  # Initial plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T07:12:46.176188Z",
     "iopub.status.busy": "2025-01-23T07:12:46.175937Z",
     "iopub.status.idle": "2025-01-23T07:12:46.181328Z",
     "shell.execute_reply": "2025-01-23T07:12:46.180447Z",
     "shell.execute_reply.started": "2025-01-23T07:12:46.176163Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def scroll(image_np, normalize_entire=True):\n",
    "    if len(image_np.shape) == 3:\n",
    "        image_np = np.expand_dims(image_np, axis=0)\n",
    "    max_slice_index = image_np.shape[1] - 1\n",
    "    global slice_slider\n",
    "    slice_slider = widgets.IntSlider(min=0, max=max_slice_index, step=1, value=0)\n",
    "   \n",
    "    def on_value_change(change):\n",
    "        plot_slice(image_np, change['new'], normalize_entire=normalize_entire)\n",
    "   \n",
    "    slice_slider.observe(on_value_change, names='value')\n",
    "    display(slice_slider)  # Initial display of the slider\n",
    "    plot_slice(image_np, slice_slider.value, normalize_entire=normalize_entire)  # Initial plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T07:12:51.805785Z",
     "iopub.status.busy": "2025-01-23T07:12:51.805459Z",
     "iopub.status.idle": "2025-01-23T07:12:51.810160Z",
     "shell.execute_reply": "2025-01-23T07:12:51.809383Z",
     "shell.execute_reply.started": "2025-01-23T07:12:51.805755Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def threshold(scores,threshold=0.50, minimum=0, maximum = 1.0):\n",
    "    x = np.array(list(scores))\n",
    "    x[x >= threshold] = maximum\n",
    "    x[x < threshold] = minimum\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T07:12:52.003788Z",
     "iopub.status.busy": "2025-01-23T07:12:52.003543Z",
     "iopub.status.idle": "2025-01-23T07:12:52.008996Z",
     "shell.execute_reply": "2025-01-23T07:12:52.008179Z",
     "shell.execute_reply.started": "2025-01-23T07:12:52.003764Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "  plt.figure(figsize=(16,9))\n",
    "  cm = confusion_matrix(y_true, y_pred)\n",
    "  ax= plt.subplot()\n",
    "  sns.heatmap(cm, annot=True, fmt='g', ax=ax, annot_kws={\"size\": 20})\n",
    "\n",
    "  # labels, title and ticks\n",
    "  ax.set_xlabel('Predicted labels', fontsize=20)\n",
    "  ax.set_ylabel('True labels', fontsize=20)\n",
    "  ax.set_title('Confusion Matrix', fontsize=20)\n",
    "  ax.xaxis.set_ticklabels(['pCR','Non pCR'], fontsize=20)\n",
    "  ax.yaxis.set_ticklabels(['pCR','Non pCR'], fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T07:12:52.144672Z",
     "iopub.status.busy": "2025-01-23T07:12:52.144084Z",
     "iopub.status.idle": "2025-01-23T07:12:52.149113Z",
     "shell.execute_reply": "2025-01-23T07:12:52.148279Z",
     "shell.execute_reply.started": "2025-01-23T07:12:52.144646Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_loss(epoch_train_loss, epoch_val_loss, lr, bs):\n",
    "  plt.figure(figsize=(16,9))\n",
    "  plt.plot(epoch_train_loss, c='b', label='Training loss')\n",
    "  plt.plot(epoch_val_loss, c='r', label = 'Testing loss')\n",
    "  plt.legend()\n",
    "  plt.grid()\n",
    "  plt.xlabel('Epochs', fontsize=20)\n",
    "  plt.ylabel('Loss', fontsize=20)\n",
    "  plt.title(f'Learning Rate: {lr} Batch Size: {bs}')\n",
    "  plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T07:12:52.287320Z",
     "iopub.status.busy": "2025-01-23T07:12:52.287051Z",
     "iopub.status.idle": "2025-01-23T07:12:52.291736Z",
     "shell.execute_reply": "2025-01-23T07:12:52.290885Z",
     "shell.execute_reply.started": "2025-01-23T07:12:52.287297Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_accuracy(epoch_train_accuracy, epoch_val_accuracy, lr, bs):\n",
    "  plt.figure(figsize=(16,9))\n",
    "  plt.plot(epoch_train_accuracy, c='b', label='Training accuracy')\n",
    "  plt.plot(epoch_val_accuracy, c='r', label = 'Testing accuracy')\n",
    "  plt.legend()\n",
    "  plt.grid()\n",
    "  plt.xlabel('Epochs', fontsize=20)\n",
    "  plt.ylabel('Accuracy', fontsize=20)\n",
    "  plt.title(f'Learning Rate: {lr} Batch Size: {bs}')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T07:12:52.524112Z",
     "iopub.status.busy": "2025-01-23T07:12:52.523737Z",
     "iopub.status.idle": "2025-01-23T07:12:52.528825Z",
     "shell.execute_reply": "2025-01-23T07:12:52.527998Z",
     "shell.execute_reply.started": "2025-01-23T07:12:52.524056Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_auc(epoch_train_auc, epoch_val_auc, lr, bs):\n",
    "  plt.figure(figsize=(16,9))\n",
    "  plt.plot(epoch_train_auc, c='b', label='Training AUC')\n",
    "  plt.plot(epoch_val_auc, c='r', label = 'Testing AUC')\n",
    "  plt.legend()\n",
    "  plt.grid()\n",
    "  plt.xlabel('Epochs', fontsize=20)\n",
    "  plt.ylabel('AUC', fontsize=20)\n",
    "  plt.title(f'Learning Rate: {lr} Batch Size: {bs}')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T07:12:56.283127Z",
     "iopub.status.busy": "2025-01-23T07:12:56.282832Z",
     "iopub.status.idle": "2025-01-23T07:12:56.295157Z",
     "shell.execute_reply": "2025-01-23T07:12:56.294254Z",
     "shell.execute_reply.started": "2025-01-23T07:12:56.283102Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def bootstrap_auc_ci(y_true, y_score, n_bootstraps=1000, alpha=0.95, random_state=None):\n",
    "    \"\"\"\n",
    "    Compute AUC and its (alpha*100)% confidence interval via bootstrapping.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert to numpy arrays in case they aren't\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_score = np.asarray(y_score)\n",
    "\n",
    "    # Initialize random state\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    \n",
    "    # Calculate the true AUC on the full dataset\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    auc_original = auc(fpr, tpr)\n",
    "\n",
    "    # Perform bootstrapping\n",
    "    bootstrapped_scores = []\n",
    "    for _ in range(n_bootstraps):\n",
    "        # Sample with replacement\n",
    "        indices = rng.integers(0, len(y_score), len(y_score))\n",
    "        # Ensure we have at least one positive and one negative\n",
    "        if len(np.unique(y_true[indices])) < 2:\n",
    "            continue\n",
    "        fpr_bs, tpr_bs, _ = roc_curve(y_true[indices], y_score[indices])\n",
    "        score = auc(fpr_bs, tpr_bs)\n",
    "        bootstrapped_scores.append(score)\n",
    "    \n",
    "    # Sort the scores\n",
    "    sorted_scores = np.array(bootstrapped_scores)\n",
    "    sorted_scores.sort()\n",
    "\n",
    "    # Confidence interval boundaries\n",
    "    lower_idx = int((1.0 - alpha) / 2 * len(sorted_scores))\n",
    "    upper_idx = int((1.0 + alpha) / 2 * len(sorted_scores))\n",
    "    lower_bound = sorted_scores[lower_idx]\n",
    "    upper_bound = sorted_scores[upper_idx]\n",
    "\n",
    "    return auc_original, lower_bound, upper_bound\n",
    "\n",
    "def plot_roc_curve(train_targets, train_probs, val_targets, val_probs, n_bootstraps=1000, alpha=0.95):\n",
    "    # Convert inputs to NumPy arrays if needed\n",
    "    train_targets = np.asarray(train_targets)\n",
    "    train_probs   = np.asarray(train_probs)\n",
    "    val_targets   = np.asarray(val_targets)\n",
    "    val_probs     = np.asarray(val_probs)\n",
    "    \n",
    "    # Compute AUCs and 95% CI\n",
    "    roc_auc_train, lower_train, upper_train = bootstrap_auc_ci(\n",
    "        train_targets, train_probs, n_bootstraps=n_bootstraps, alpha=alpha, random_state=42\n",
    "    )\n",
    "    roc_auc_val, lower_val, upper_val = bootstrap_auc_ci(\n",
    "        val_targets, val_probs, n_bootstraps=n_bootstraps, alpha=alpha, random_state=42\n",
    "    )\n",
    "\n",
    "    print(f\"Training AUC = {roc_auc_train:.3f} (95% CI: {lower_train:.3f} - {upper_train:.3f})\")\n",
    "    print(f\"Validation AUC = {roc_auc_val:.3f} (95% CI: {lower_val:.3f} - {upper_val:.3f})\")\n",
    "\n",
    "    # Compute ROC curves for plotting\n",
    "    fpr_train, tpr_train, _ = roc_curve(train_targets, train_probs)\n",
    "    fpr_val, tpr_val, thresholds_val = roc_curve(val_targets, val_probs)\n",
    "\n",
    "    # Calculate Youden's J for the validation set\n",
    "    youden_j = tpr_val - fpr_val\n",
    "    optimal_idx = np.argmax(youden_j)\n",
    "    optimal_threshold = thresholds_val[optimal_idx]\n",
    "    optimal_point = (fpr_val[optimal_idx], tpr_val[optimal_idx])\n",
    "    print(f\"Optimal Threshold (Youden's J) = {optimal_threshold:.4f}\")\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(\n",
    "        fpr_train, tpr_train, color='blue', lw=2,\n",
    "        label=f'Training ROC (AUC = {roc_auc_train:.3f}, 95% CI: {lower_train:.3f}-{upper_train:.3f})'\n",
    "    )\n",
    "    plt.plot(\n",
    "        fpr_val, tpr_val, color='red', lw=2,\n",
    "        label=f'Testing ROC (AUC = {roc_auc_val:.3f}, 95% CI: {lower_val:.3f}-{upper_val:.3f})'\n",
    "    )\n",
    "    plt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--')\n",
    "    plt.scatter(optimal_point[0], optimal_point[1], marker='o', color='green', label='Optimal Threshold')\n",
    "\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=14)\n",
    "    plt.ylabel('True Positive Rate', fontsize=14)\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curves', fontsize=16)\n",
    "    plt.legend(loc=\"lower right\", fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T07:12:56.499924Z",
     "iopub.status.busy": "2025-01-23T07:12:56.499647Z",
     "iopub.status.idle": "2025-01-23T07:12:56.506699Z",
     "shell.execute_reply": "2025-01-23T07:12:56.505887Z",
     "shell.execute_reply.started": "2025-01-23T07:12:56.499898Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def last_fm_neurons(model, data, device):\n",
    "    model_children = list(model.children())  # Sequential children of the model\n",
    "\n",
    "    layers = []\n",
    "\n",
    "    # Collect all layers from the cnn_model (1st Sequential)\n",
    "    for child in model_children:\n",
    "        if isinstance(child, nn.Sequential):\n",
    "            for layer in child.children():\n",
    "                if not isinstance(layer, nn.Dropout):\n",
    "                    layers.append(layer)\n",
    "            break\n",
    "\n",
    "    # Add the conv2d_layer (2nd Sequential)\n",
    "    for child in model_children:\n",
    "        if isinstance(child, nn.Sequential):\n",
    "            for layer in child.children():\n",
    "                if not isinstance(layer, nn.Dropout):\n",
    "                    layers.append(layer)\n",
    "\n",
    "    img = data[0]['image']  # Grab a random image\n",
    "    img = torch.from_numpy(img).to(device)  # Convert image to GPU tensor and move it to the specified device\n",
    "\n",
    "    # Ensure the image is in float32 format\n",
    "    img = img.float()\n",
    "\n",
    "    img = img.unsqueeze(0)  # Change shape of image to be (1, C, D, H, W); 1 means we have 1 image\n",
    "\n",
    "    # Run the image through the model layers sequentially\n",
    "    results = [layers[0](img)]\n",
    "\n",
    "    for i in range(1, len(layers)):\n",
    "        results.append(layers[i](results[-1]))\n",
    "\n",
    "    feature_maps = results\n",
    "\n",
    "    # Return the shape of the final feature map from the Conv2d layer and its flattened shape\n",
    "    last_fm_shape = feature_maps[-1].shape\n",
    "    last_fm_flattened_shape = feature_maps[-1].view(-1).shape\n",
    "\n",
    "    return last_fm_shape, last_fm_flattened_shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T07:12:56.704522Z",
     "iopub.status.busy": "2025-01-23T07:12:56.703891Z",
     "iopub.status.idle": "2025-01-23T07:12:56.708659Z",
     "shell.execute_reply": "2025-01-23T07:12:56.707731Z",
     "shell.execute_reply.started": "2025-01-23T07:12:56.704495Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_in_features(model, input_shape, device):\n",
    "    # Create a dummy input tensor with the given shape\n",
    "    dummy_input = torch.zeros(1, *input_shape).to(device)\n",
    "    \n",
    "    # Move model to the specified device\n",
    "    model.to(device)\n",
    "    \n",
    "    # Forward pass the dummy input through the model up to the last conv2d layer\n",
    "    with torch.no_grad():\n",
    "        x = model.cnn_model(dummy_input)\n",
    "\n",
    "    return x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T07:12:56.855574Z",
     "iopub.status.busy": "2025-01-23T07:12:56.854995Z",
     "iopub.status.idle": "2025-01-23T07:12:56.859929Z",
     "shell.execute_reply": "2025-01-23T07:12:56.859000Z",
     "shell.execute_reply.started": "2025-01-23T07:12:56.855548Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_class_weights(data):\n",
    "    labels = [d['label'].item() for d in data]  # Extract labels from the dataset\n",
    "    labels = np.array(labels).astype(int)  # Ensure labels are integers\n",
    "    \n",
    "    # Count the frequency of each class\n",
    "    class_counts = np.bincount(labels)\n",
    "    \n",
    "    # Calculate class weights: total_samples / (num_classes * class_counts)\n",
    "    total_samples = len(labels)\n",
    "    class_weights = total_samples / (len(class_counts) * class_counts)\n",
    "    \n",
    "    return torch.tensor(class_weights, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T07:13:00.291667Z",
     "iopub.status.busy": "2025-01-23T07:13:00.290983Z",
     "iopub.status.idle": "2025-01-23T07:13:00.303978Z",
     "shell.execute_reply": "2025-01-23T07:13:00.303031Z",
     "shell.execute_reply.started": "2025-01-23T07:13:00.291630Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def fit_km(df, outcomes_label, predictor, group_name_mapping, color_palette):\n",
    "    for OUTCOME, (label, flip) in outcomes_label.items():\n",
    "        # Select relevant columns based on the outcome label (PRE exclusion)\n",
    "        if label:\n",
    "            df_filtered = df[df[f'PRE_{OUTCOME}'] == 0]\n",
    "            df_selected = df_filtered[[OUTCOME, f'{OUTCOME}_TIME', predictor]]\n",
    "        else:\n",
    "            df_selected = df[[OUTCOME, f'{OUTCOME}_TIME', predictor]]\n",
    "\n",
    "        plt.figure(figsize=(10, 9))\n",
    "\n",
    "        # Dictionary to store Kaplan-Meier fitters for each group\n",
    "        kmf_group_dict = {}\n",
    "\n",
    "        # Prepare lists to store survival times and events for the log-rank test\n",
    "        times = []\n",
    "        events = []\n",
    "        groups = []\n",
    "\n",
    "        # Iterate over the groups in the order of group_name_mapping\n",
    "        for group in group_name_mapping.keys():\n",
    "            if group in df_selected[predictor].unique():\n",
    "                group_data = df_selected[df_selected[predictor] == group]\n",
    "                kmf_group = KaplanMeierFitter()\n",
    "                kmf_group.fit(\n",
    "                    group_data[f'{OUTCOME}_TIME'],\n",
    "                    event_observed=group_data[OUTCOME],\n",
    "                    label=group_name_mapping[group]\n",
    "                )\n",
    "\n",
    "                # Retrieve color and line style for the current group\n",
    "                color, linestyle = color_palette.get(group, ('blue', 'solid'))  # Default values if group not in color_palette\n",
    "\n",
    "                if flip:\n",
    "                    # For cumulative incidence (1 - survival probability)\n",
    "                    survival_prob = kmf_group.survival_function_\n",
    "                    ci_upper = kmf_group.confidence_interval_.iloc[:, 1]\n",
    "                    ci_lower = kmf_group.confidence_interval_.iloc[:, 0]\n",
    "\n",
    "                    plt.plot(\n",
    "                        survival_prob.index,\n",
    "                        1 - survival_prob.values.flatten(),\n",
    "                        label=group_name_mapping[group],\n",
    "                        color=color,\n",
    "                        linewidth=2,\n",
    "                        linestyle=linestyle\n",
    "                    )\n",
    "\n",
    "                    # Add confidence intervals for cumulative incidence\n",
    "                    plt.fill_between(\n",
    "                        survival_prob.index,\n",
    "                        1 - ci_upper,\n",
    "                        1 - ci_lower,\n",
    "                        color=color,\n",
    "                        alpha=0.3\n",
    "                    )\n",
    "                else:\n",
    "                    # Plot survival curve with specified color and line style\n",
    "                    kmf_group.plot_survival_function(\n",
    "                        ci_show=False,\n",
    "                        color=color,\n",
    "                        linewidth=2,\n",
    "                        linestyle=linestyle\n",
    "                    )\n",
    "\n",
    "                kmf_group_dict[group] = kmf_group\n",
    "\n",
    "                # Collect data for log-rank test\n",
    "                times.extend(group_data[f'{OUTCOME}_TIME'])\n",
    "                events.extend(group_data[OUTCOME])\n",
    "                groups.extend([group] * len(group_data))\n",
    "\n",
    "        # Perform log-rank test\n",
    "        if len(set(groups)) > 2:\n",
    "            results = multivariate_logrank_test(times, groups, events)\n",
    "        else:\n",
    "            unique_groups = list(set(groups))\n",
    "            group_1 = df_selected[df_selected[predictor] == unique_groups[0]]\n",
    "            group_2 = df_selected[df_selected[predictor] == unique_groups[1]]\n",
    "            results = logrank_test(\n",
    "                group_1[f'{OUTCOME}_TIME'],\n",
    "                group_2[f'{OUTCOME}_TIME'],\n",
    "                event_observed_A=group_1[OUTCOME],\n",
    "                event_observed_B=group_2[OUTCOME]\n",
    "            )\n",
    "\n",
    "        # Output p-value\n",
    "        print(f\"Log-rank test p-value for {OUTCOME}: {results.p_value}\")\n",
    "\n",
    "        # Set the x-axis to intervals of 1\n",
    "        plt.xticks(ticks=range(0, int(df_selected[f'{OUTCOME}_TIME'].max()) + 1, 1))\n",
    "\n",
    "        # Set y-axis limits and enable grid\n",
    "        plt.ylim(0.5, 1)\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Add legend in the top right\n",
    "        plt.legend(loc=\"upper right\")\n",
    "\n",
    "        # Add titles and labels\n",
    "        plt.xlabel('Time Since Breast Cancer Diagnosis (Years)')\n",
    "        plt.ylabel('Recurrence-Free Survival Probability' if not flip else 'Cumulative Incidence')\n",
    "\n",
    "        # Adding number at risk and censored information at the bottom with group names\n",
    "        at_risk_labels = [group_name_mapping[g] for g in group_name_mapping.keys() if g in kmf_group_dict]\n",
    "        add_at_risk_counts(*[kmf_group_dict[g] for g in group_name_mapping.keys() if g in kmf_group_dict], labels=at_risk_labels)\n",
    "\n",
    "        # Adjust layout\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T07:13:00.505486Z",
     "iopub.status.busy": "2025-01-23T07:13:00.504943Z",
     "iopub.status.idle": "2025-01-23T07:13:00.515262Z",
     "shell.execute_reply": "2025-01-23T07:13:00.514416Z",
     "shell.execute_reply.started": "2025-01-23T07:13:00.505458Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def bootstrap_c_index(model, df, n_bootstrap=1000, random_seed=42):\n",
    "    \"\"\"\n",
    "    Compute the bootstrap 95% CI for the C-index.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    durations = df[model.duration_col].values\n",
    "    events    = df[model.event_col].values\n",
    "    \n",
    "    # Predicted partial hazard\n",
    "    preds = model.predict_partial_hazard(df).values\n",
    "    \n",
    "    # Negate so that larger is interpreted as \"later event\" by concordance_index\n",
    "    preds = -preds\n",
    "\n",
    "    c_index_list = []\n",
    "    n = len(df)\n",
    "    \n",
    "    for _ in range(n_bootstrap):\n",
    "        idx = np.random.choice(n, size=n, replace=True)\n",
    "        c_index_list.append(concordance_index(durations[idx],\n",
    "                                              preds[idx],\n",
    "                                              events[idx]))\n",
    "    mean_c = np.mean(c_index_list)\n",
    "    lower  = np.percentile(c_index_list, 2.5)\n",
    "    upper  = np.percentile(c_index_list, 97.5)\n",
    "    \n",
    "    return mean_c, lower, upper\n",
    "\n",
    "def fit_cox_model(df, outcomes_label, predictors, \n",
    "                  bootstrap=True, n_bootstrap=1000):\n",
    "    \"\"\"\n",
    "    Fit Cox models for each outcome, then print the summary (HR, 95% CI, p),\n",
    "    plus the C-index for each model. Optionally, bootstrap to get 95% CI\n",
    "    for the C-index.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    models_dict : dict\n",
    "        Dictionary of {OUTCOME: fitted CoxPHFitter objects}.\n",
    "        You can use these objects later for c-index comparison.\n",
    "    \"\"\"\n",
    "    \n",
    "    models_dict = {}  # will hold the fitted models\n",
    "    \n",
    "    for OUTCOME, label in outcomes_label.items():\n",
    "        \n",
    "        # Select relevant columns\n",
    "        if label:\n",
    "            df_filtered = df[df[f'PRE_{OUTCOME}'] == 0]\n",
    "            df_selected = df_filtered[[OUTCOME, f'{OUTCOME}_TIME'] + predictors]\n",
    "        else:\n",
    "            df_selected = df[[OUTCOME, f'{OUTCOME}_TIME'] + predictors]\n",
    "        \n",
    "        # Fit Cox proportional hazards model\n",
    "        cph = CoxPHFitter()\n",
    "        cph.fit(df_selected, duration_col=f'{OUTCOME}_TIME', event_col=OUTCOME)\n",
    "\n",
    "        # Print the summary results\n",
    "        summary = cph.summary[['exp(coef)', 'exp(coef) lower 95%', \n",
    "                               'exp(coef) upper 95%', 'p']]\n",
    "        summary.columns = ['Hazard Ratio', 'CI Lower 95%', 'CI Upper 95%', 'p-value']\n",
    "\n",
    "        print(f\"Results for {OUTCOME}:\")\n",
    "        for covariate, row in summary.iterrows():\n",
    "            hazard_ratio = f\"{row['Hazard Ratio']:.2f}\"\n",
    "            lower_ci     = f\"{row['CI Lower 95%']:.2f}\"\n",
    "            upper_ci     = f\"{row['CI Upper 95%']:.2f}\"\n",
    "            p_value      = \"<0.005\" if row['p-value'] < 0.005 else f\"{row['p-value']:.2g}\"\n",
    "            print(f\"{covariate}\\t{hazard_ratio} [{lower_ci}, {upper_ci}]\\t{p_value}\")\n",
    "        \n",
    "        # Single-run (in-sample) C-index\n",
    "        print(\"\\nSingle-run C-index:\", f\"{cph.concordance_index_:.3f}\")\n",
    "        \n",
    "        # Optionally do bootstrap\n",
    "        if bootstrap:\n",
    "            mean_c, lower, upper = bootstrap_c_index(\n",
    "                model=cph, \n",
    "                df=df_selected, \n",
    "                n_bootstrap=n_bootstrap\n",
    "            )\n",
    "            print(f\"Bootstrap C-index: {mean_c:.3f} (95% CI: [{lower:.3f}, {upper:.3f}])\")\n",
    "        \n",
    "        # Store the fitted model in the dictionary\n",
    "        models_dict[OUTCOME] = cph\n",
    "        \n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "    # Return the dictionary of models\n",
    "    return models_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T07:13:00.686136Z",
     "iopub.status.busy": "2025-01-23T07:13:00.685869Z",
     "iopub.status.idle": "2025-01-23T07:13:00.694462Z",
     "shell.execute_reply": "2025-01-23T07:13:00.693656Z",
     "shell.execute_reply.started": "2025-01-23T07:13:00.686110Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def bootstrap_compare_c_indices(modelA, modelB, df, \n",
    "                                duration_col='RD', event_col='RD_TIME',\n",
    "                                n_bootstrap=1000, random_seed=42,\n",
    "                                flip_sign=True):\n",
    "    \"\"\"\n",
    "    Compare the C-index of two fitted Cox models on the same dataset\n",
    "    using bootstrap resampling (paired). Returns a dict with the single-run\n",
    "    C-index for each model, bootstrap mean C-index, difference, 95% CI, \n",
    "    and an approximate two-sided p-value for that difference.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from lifelines.utils import concordance_index\n",
    "\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    durations = df[duration_col].values\n",
    "    events    = df[event_col].values\n",
    "\n",
    "    # predictions\n",
    "    phA = modelA.predict_partial_hazard(df).values\n",
    "    phB = modelB.predict_partial_hazard(df).values\n",
    "    \n",
    "    # Possibly flip sign if needed\n",
    "    predsA = -phA if flip_sign else phA\n",
    "    predsB = -phB if flip_sign else phB\n",
    "\n",
    "    # Single-run c-indices\n",
    "    cA = concordance_index(durations, predsA, events)\n",
    "    cB = concordance_index(durations, predsB, events)\n",
    "\n",
    "    cA_values = []\n",
    "    cB_values = []\n",
    "    diffs     = []\n",
    "    \n",
    "    n = len(df)\n",
    "\n",
    "    for _ in range(n_bootstrap):\n",
    "        idx = np.random.choice(n, size=n, replace=True)\n",
    "        cA_boot = concordance_index(durations[idx], predsA[idx], events[idx])\n",
    "        cB_boot = concordance_index(durations[idx], predsB[idx], events[idx])\n",
    "        cA_values.append(cA_boot)\n",
    "        cB_values.append(cB_boot)\n",
    "        diffs.append(cA_boot - cB_boot)\n",
    "\n",
    "    cA_values = np.array(cA_values)\n",
    "    cB_values = np.array(cB_values)\n",
    "    diffs     = np.array(diffs)\n",
    "\n",
    "    mean_diff = np.mean(diffs)\n",
    "    ci_lower  = np.percentile(diffs, 2.5)\n",
    "    ci_upper  = np.percentile(diffs, 97.5)\n",
    "    \n",
    "    # Approximate two-sided p-value\n",
    "    frac_lt0 = np.mean(diffs < 0)\n",
    "    frac_gt0 = np.mean(diffs > 0)\n",
    "    p_value  = 2 * min(frac_lt0, frac_gt0)\n",
    "    p_value  = min(p_value, 1.0)\n",
    "\n",
    "    # Round to 3 decimals\n",
    "    return {\n",
    "        'CindexA': round(cA, 3),\n",
    "        'CindexB': round(cB, 3),\n",
    "        'mean_diff': round(mean_diff, 3),\n",
    "        'ci_lower': round(ci_lower, 3),\n",
    "        'ci_upper': round(ci_upper, 3),\n",
    "        'p_value': round(p_value, 3),\n",
    "        'mean_cA': round(cA_values.mean(), 3),\n",
    "        'mean_cB': round(cB_values.mean(), 3)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T07:13:00.868217Z",
     "iopub.status.busy": "2025-01-23T07:13:00.867985Z",
     "iopub.status.idle": "2025-01-23T07:13:00.872625Z",
     "shell.execute_reply": "2025-01-23T07:13:00.871753Z",
     "shell.execute_reply.started": "2025-01-23T07:13:00.868189Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # For multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T07:13:09.442412Z",
     "iopub.status.busy": "2025-01-23T07:13:09.442069Z",
     "iopub.status.idle": "2025-01-23T07:13:09.457946Z",
     "shell.execute_reply": "2025-01-23T07:13:09.457041Z",
     "shell.execute_reply.started": "2025-01-23T07:13:09.442380Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, data, device, \n",
    "                lr=1e-4, epochs=100, batch_size=32, weight_decay=1e-3, \n",
    "                early_stopping_patience=10, lr_scheduler=True, \n",
    "                use_amp=True, num_workers=4):\n",
    "\n",
    "    # Compute class weights\n",
    "    train_labels = data.train_labels\n",
    "    classes, class_counts = np.unique(train_labels, return_counts=True)\n",
    "    class_weights = torch.tensor(len(train_labels) / (len(classes) * class_counts), dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Use BCEWithLogitsLoss instead of BCELoss (for numerical stability with AMP)\n",
    "    criterion = nn.BCEWithLogitsLoss(reduction='none')\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    # Learning Rate Scheduler (Reduce LR when validation loss stagnates)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5) if lr_scheduler else None\n",
    "    \n",
    "    # Early Stopping Setup\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # AMP Scaler (Updated syntax)\n",
    "    scaler = amp.GradScaler() if use_amp else None\n",
    "    \n",
    "    # Tracking metrics\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "    train_aucs, val_aucs = [], []\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # === Training Phase ===\n",
    "        model.train()\n",
    "        data.set_mode('train')\n",
    "        train_loader = DataLoader(data, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "        \n",
    "        train_correct, train_total, train_probs, train_targets = 0, 0, [], []\n",
    "        epoch_train_losses = []\n",
    "\n",
    "        for D in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            image, label = D['image'].to(device).float(), D['label'].to(device).float().view(-1, 1)\n",
    "            \n",
    "            with amp.autocast(enabled=use_amp):  # Updated syntax for AMP\n",
    "                y_hat = model(image)  # No sigmoid in forward pass!\n",
    "                sample_weights = class_weights[label.long()].view(-1, 1)\n",
    "                loss = (criterion(y_hat, label) * sample_weights).mean()\n",
    "\n",
    "            if use_amp:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            epoch_train_losses.append(loss.item())\n",
    "            train_correct += (y_hat.sigmoid().round() == label).sum().item()  # Apply sigmoid only for prediction\n",
    "            train_total += label.size(0)\n",
    "            train_probs.extend(y_hat.sigmoid().detach().cpu().numpy().flatten())  # Convert logits to probabilities\n",
    "            train_targets.extend(label.detach().cpu().numpy().flatten())\n",
    "\n",
    "        train_auc = roc_auc_score(train_targets, train_probs)\n",
    "        train_losses.append(np.mean(epoch_train_losses))\n",
    "        train_accs.append(train_correct / train_total)\n",
    "        train_aucs.append(train_auc)\n",
    "\n",
    "        # === Validation Phase ===\n",
    "        model.eval()\n",
    "        data.set_mode('val')\n",
    "        val_loader = DataLoader(data, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "        \n",
    "        val_correct, val_total, val_probs, val_targets = 0, 0, [], []\n",
    "        epoch_val_losses = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for D in val_loader:\n",
    "                image, label = D['image'].to(device).float(), D['label'].to(device).float().view(-1, 1)\n",
    "                \n",
    "                with amp.autocast(enabled=use_amp):\n",
    "                    y_hat = model(image)  # No sigmoid in forward pass!\n",
    "                    sample_weights = class_weights[label.long()].view(-1, 1)\n",
    "                    loss = (criterion(y_hat, label) * sample_weights).mean()\n",
    "\n",
    "                epoch_val_losses.append(loss.item())\n",
    "                val_correct += (y_hat.sigmoid().round() == label).sum().item()\n",
    "                val_total += label.size(0)\n",
    "                val_probs.extend(y_hat.sigmoid().cpu().numpy().flatten())\n",
    "                val_targets.extend(label.cpu().numpy().flatten())\n",
    "\n",
    "        val_auc = roc_auc_score(val_targets, val_probs)\n",
    "        val_losses.append(np.mean(epoch_val_losses))\n",
    "        val_accs.append(val_correct / val_total)\n",
    "        val_aucs.append(val_auc)\n",
    "\n",
    "        # Learning Rate Scheduling\n",
    "        if lr_scheduler:\n",
    "            scheduler.step(val_losses[-1])\n",
    "\n",
    "        # Early Stopping\n",
    "        if val_losses[-1] < best_val_loss:\n",
    "            best_val_loss = val_losses[-1]\n",
    "            patience_counter = 0  # Reset patience counter\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")  # Save best model\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "        # Print Progress\n",
    "        if epoch % 10 == 0 or epoch == epochs:\n",
    "            print(f\"Epoch {epoch} | Train Loss: {train_losses[-1]:.4f} | Val Loss: {val_losses[-1]:.4f} | \"\n",
    "                  f\"Train Acc: {100 * train_accs[-1]:.2f}% | Val Acc: {100 * val_accs[-1]:.2f}% | \"\n",
    "                  f\"Train AUC: {train_auc:.4f} | Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "    # Plot Metrics\n",
    "    plot_loss(train_losses, val_losses, lr, batch_size)\n",
    "    plot_accuracy(train_accs, val_accs, lr, batch_size)\n",
    "    plot_auc(train_aucs, val_aucs, lr, batch_size)\n",
    "    plot_roc_curve(train_targets, train_probs, val_targets, val_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T07:13:10.040366Z",
     "iopub.status.busy": "2025-01-23T07:13:10.039838Z",
     "iopub.status.idle": "2025-01-23T07:13:10.046164Z",
     "shell.execute_reply": "2025-01-23T07:13:10.045281Z",
     "shell.execute_reply.started": "2025-01-23T07:13:10.040336Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, data, device):\n",
    "    data.mode = 'val'\n",
    "    val_dataloader = DataLoader(data, shuffle=False, batch_size=2, num_workers=0)\n",
    "    model.eval()\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for D in val_dataloader:\n",
    "            images = D['image'].to(device)\n",
    "            labels = D['label'].to(device).float().view(-1, 1)\n",
    "\n",
    "            outputs = model(images)\n",
    "            predictions = (outputs >= 0.5).float()\n",
    "\n",
    "            y_true.append(labels.cpu().numpy())\n",
    "            y_pred.append(predictions.cpu().numpy())\n",
    "\n",
    "    y_true = np.concatenate(y_true, axis=0).squeeze()\n",
    "    y_pred = np.concatenate(y_pred, axis=0).squeeze()\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    return y_true, y_pred, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, data, device, lrng_rt=0.0001, EPOCH=200, batch_size=4, weight_decay=1e-4):\n",
    "    \n",
    "    # Compute class weights based on the training data\n",
    "    train_labels = data.train_labels  # Assuming this is a NumPy array of labels in the training set\n",
    "    classes = np.unique(train_labels)\n",
    "    class_counts = np.array([(train_labels == c).sum() for c in classes])\n",
    "    total_samples = len(train_labels)\n",
    "    num_classes = len(classes)\n",
    "\n",
    "    # Compute weights inversely proportional to class frequencies\n",
    "    class_weights = total_samples / (num_classes * class_counts)\n",
    "    class_weights = class_weights / class_weights.sum()  # Normalize weights\n",
    "\n",
    "    # Convert class weights to a PyTorch tensor\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Initialize the loss function with no reduction\n",
    "    error = nn.BCELoss(reduction='none')\n",
    "\n",
    "    epoch_train_loss = []\n",
    "    epoch_val_loss = []\n",
    "    epoch_train_accuracy = []\n",
    "    epoch_val_accuracy = []\n",
    "    epoch_train_auc = []\n",
    "    epoch_val_auc = []\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lrng_rt, weight_decay=weight_decay)\n",
    "\n",
    "    for epoch in range(1, EPOCH + 1):\n",
    "        train_losses = []\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        train_probs = []\n",
    "        train_targets = []\n",
    "\n",
    "        data.set_mode('train')  # Ensure the dataset is in training mode\n",
    "        train_dataloader = DataLoader(data, shuffle=True, batch_size=batch_size, num_workers=0)\n",
    "        model.train()\n",
    "\n",
    "        for D in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            image = D['image'].to(device).float()\n",
    "            label = D['label'].to(device).float()\n",
    "\n",
    "            y_hat = model(image)\n",
    "            label = label.view(-1, 1)\n",
    "\n",
    "            # Compute the per-sample loss\n",
    "            loss = error(y_hat, label)\n",
    "\n",
    "            # Get the weights for each sample in the batch\n",
    "            label_flat = label.view(-1)\n",
    "            sample_weights = class_weights[label_flat.long()].view(-1, 1)\n",
    "\n",
    "            # Apply the weights to the loss\n",
    "            weighted_loss = loss * sample_weights\n",
    "\n",
    "            # Compute the mean loss over the batch\n",
    "            loss = weighted_loss.mean()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "            predicted = (y_hat > 0.5).float()\n",
    "            train_correct += (predicted == label).sum().item()\n",
    "            train_total += label.size(0)\n",
    "\n",
    "            # Store probabilities and targets for AUC calculation\n",
    "            train_probs.extend(y_hat.detach().cpu().numpy().flatten())\n",
    "            train_targets.extend(label.detach().cpu().numpy().flatten())\n",
    "\n",
    "        # Compute AUC for the training set\n",
    "        train_auc = roc_auc_score(train_targets, train_probs)\n",
    "        epoch_train_auc.append(train_auc)\n",
    "\n",
    "        epoch_train_loss.append(np.mean(train_losses))\n",
    "        epoch_train_accuracy.append(train_correct / train_total)\n",
    "\n",
    "        # Validate the model\n",
    "        val_losses = []\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        val_probs = []\n",
    "        val_targets = []\n",
    "\n",
    "        data.set_mode('val')  # Ensure the dataset is in validation mode\n",
    "        val_dataloader = DataLoader(data, shuffle=False, batch_size=batch_size, num_workers=0)\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for D in val_dataloader:\n",
    "                image = D['image'].to(device).float()\n",
    "                label = D['label'].to(device).float()\n",
    "\n",
    "                y_hat = model(image)\n",
    "                label = label.view(-1, 1)\n",
    "\n",
    "                # Compute the per-sample loss\n",
    "                loss = error(y_hat, label)\n",
    "\n",
    "                # Get the weights for each sample in the batch\n",
    "                label_flat = label.view(-1)\n",
    "                sample_weights = class_weights[label_flat.long()].view(-1, 1)\n",
    "\n",
    "                # Apply the weights to the loss\n",
    "                weighted_loss = loss * sample_weights\n",
    "\n",
    "                # Compute the mean loss over the batch\n",
    "                loss = weighted_loss.mean()\n",
    "\n",
    "                val_losses.append(loss.item())\n",
    "\n",
    "                predicted = (y_hat > 0.5).float()\n",
    "                val_correct += (predicted == label).sum().item()\n",
    "                val_total += label.size(0)\n",
    "\n",
    "                # Store probabilities and targets for AUC calculation\n",
    "                val_probs.extend(y_hat.detach().cpu().numpy().flatten())\n",
    "                val_targets.extend(label.detach().cpu().numpy().flatten())\n",
    "\n",
    "        # Compute AUC for the validation set\n",
    "        val_auc = roc_auc_score(val_targets, val_probs)\n",
    "        epoch_val_auc.append(val_auc)\n",
    "\n",
    "        epoch_val_loss.append(np.mean(val_losses))\n",
    "        epoch_val_accuracy.append(val_correct / val_total)\n",
    "\n",
    "        if epoch % 10 == 0 or epoch == EPOCH:\n",
    "            print('Epoch: {}\\tTrain Loss: {:.6f}\\tVal Loss: {:.6f}\\tTrain Acc: {:.2f}%\\tVal Acc: {:.2f}%\\tTrain AUC: {:.4f}\\tVal AUC: {:.4f}'.format(\n",
    "                epoch,\n",
    "                np.mean(train_losses),\n",
    "                np.mean(val_losses),\n",
    "                100.0 * train_correct / train_total,\n",
    "                100.0 * val_correct / val_total,\n",
    "                train_auc,\n",
    "                val_auc\n",
    "            ))\n",
    "\n",
    "    # Plot loss, accuracy, and AUC over epochs\n",
    "    plot_loss(epoch_train_loss, epoch_val_loss, lrng_rt, batch_size)\n",
    "    plot_accuracy(epoch_train_accuracy, epoch_val_accuracy, lrng_rt, batch_size)\n",
    "    plot_auc(epoch_train_auc, epoch_val_auc, lrng_rt, batch_size)\n",
    "\n",
    "    # After training, plot ROC curves and find optimal threshold\n",
    "    plot_roc_curve(train_targets, train_probs, val_targets, val_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T07:13:15.068394Z",
     "iopub.status.busy": "2025-01-23T07:13:15.068051Z",
     "iopub.status.idle": "2025-01-23T07:13:15.083165Z",
     "shell.execute_reply": "2025-01-23T07:13:15.082297Z",
     "shell.execute_reply.started": "2025-01-23T07:13:15.068363Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MRI(Dataset):\n",
    "    def __init__(self, rd, dfs, sf, test=0.2):\n",
    "        self.mode = 'train'\n",
    "\n",
    "        # Labels\n",
    "        rd_label = np.ones(rd.shape[0], dtype=np.float32)\n",
    "        dfs_label = np.zeros(dfs.shape[0], dtype=np.float32)\n",
    "\n",
    "        # Combine labels\n",
    "        self.labels = np.concatenate((rd_label, dfs_label))\n",
    "\n",
    "        # Store original data\n",
    "        self.rd_original = rd  # Shape: (patients, phases, slices, H, W)\n",
    "        self.dfs_original = dfs  # Shape: (patients, phases, slices, H, W)\n",
    "\n",
    "        # Store patient IDs in the order of labels\n",
    "        self.combined_patient_ids = HRp + HRn  # Assuming HRp and HRn are available here\n",
    "\n",
    "        # Split indices for train and validation\n",
    "        self.train_val_split(sf, test)\n",
    "\n",
    "        # Preprocess data\n",
    "        self.preprocess_data()\n",
    "\n",
    "    def train_val_split(self, sf, test):\n",
    "        # Split indices into train and validation sets with stratification\n",
    "        self.train_idx, self.val_idx = train_test_split(\n",
    "            np.arange(len(self.labels)),\n",
    "            test_size=test,\n",
    "            random_state=sf,\n",
    "            shuffle=True,\n",
    "            stratify=self.labels\n",
    "        )\n",
    "        \n",
    "        # Store patient indices for train and validation sets\n",
    "        self.train_patient_indices = self.train_idx.copy()\n",
    "        self.val_patient_indices = self.val_idx.copy()\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        # Preprocess training data by expanding slices\n",
    "        self.train_images = []\n",
    "        self.train_labels = []\n",
    "\n",
    "        for idx in self.train_idx:\n",
    "            label = self.labels[idx]\n",
    "            if idx < len(self.rd_original):\n",
    "                # rd data\n",
    "                image = self.rd_original[idx]  # Shape: (phases, slices, H, W)\n",
    "            else:\n",
    "                # dfs data\n",
    "                idx_adjusted = idx - len(self.rd_original)\n",
    "                image = self.dfs_original[idx_adjusted]  # Shape: (phases, slices, H, W)\n",
    "\n",
    "            # Transpose to rearrange axes: (phases, slices, H, W) -> (slices, phases, H, W)\n",
    "            image = np.transpose(image, (1, 0, 2, 3))  # Shape: (slices, phases, H, W)\n",
    "\n",
    "            # Define transformations\n",
    "            transformations = [\n",
    "                lambda x: np.flip(x, axis=1),  # Vertical flip\n",
    "                lambda x: x,  \n",
    "                lambda x: np.flip(x, axis=2),  # Horizontal flip\n",
    "                lambda x: np.flip(x, axis=1),\n",
    "                lambda x: x,\n",
    "                lambda x: np.flip(x, axis=2),  \n",
    "                lambda x: np.flip(x, axis=1),\n",
    "                lambda x: x,\n",
    "                lambda x: np.flip(x, axis=2),\n",
    "                lambda x: x,  # No flip\n",
    "            ]\n",
    "\n",
    "            # Randomly assign transformations to slices\n",
    "            #np.random.shuffle(transformations)\n",
    "\n",
    "            # Apply transformations to slices\n",
    "            for slice_idx in range(image.shape[0]):  # Number of slices per patient\n",
    "                slice_image = image[slice_idx]  # Shape: (phases, H, W)\n",
    "\n",
    "                # Apply the corresponding transformation\n",
    "                transform = transformations[slice_idx]\n",
    "                transformed_slice = transform(slice_image)\n",
    "\n",
    "                self.train_images.append(transformed_slice)\n",
    "                self.train_labels.append(label)\n",
    "\n",
    "        # Convert to NumPy arrays for efficient indexing\n",
    "        self.train_images = np.array(self.train_images)\n",
    "        self.train_labels = np.array(self.train_labels, dtype=np.float32)\n",
    "\n",
    "        # Preprocess validation data\n",
    "        self.val_images = []\n",
    "        self.val_labels = []\n",
    "\n",
    "        for idx in self.val_idx:\n",
    "            label = self.labels[idx]\n",
    "            if idx < len(self.rd_original):\n",
    "                # rd data\n",
    "                image = self.rd_original[idx, :, (self.rd_original.shape[2]-1), :, :]  # Central slice\n",
    "            else:\n",
    "                # dfs data\n",
    "                idx_adjusted = idx - len(self.rd_original)\n",
    "                image = self.dfs_original[idx_adjusted, :, (self.dfs_original.shape[2]-1), :, :]  # Central slice\n",
    "\n",
    "            self.val_images.append(image)\n",
    "            self.val_labels.append(label)\n",
    "\n",
    "        # Convert to NumPy arrays\n",
    "        self.val_images = np.array(self.val_images)\n",
    "        self.val_labels = np.array(self.val_labels, dtype=np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.mode == 'train':\n",
    "            return len(self.train_images)\n",
    "        else:\n",
    "            return len(self.val_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == 'train':\n",
    "            image = self.train_images[idx]  # Shape: (phases, H, W)\n",
    "            label = self.train_labels[idx]\n",
    "        else:\n",
    "            image = self.val_images[idx]  # Shape: (phases, H, W)\n",
    "            label = self.val_labels[idx]\n",
    "\n",
    "        # Optionally, apply any additional transformations here (e.g., normalization)\n",
    "\n",
    "        return {'image': image, 'label': label}\n",
    "\n",
    "    def set_mode(self, mode):\n",
    "        if mode in ['train', 'val']:\n",
    "            self.mode = mode\n",
    "        else:\n",
    "            raise ValueError(\"Mode should be 'train' or 'val'\")\n",
    "\n",
    "    def get_train_rd_count(self):\n",
    "        \"\"\"Returns the number of rd samples in the training set.\"\"\"\n",
    "        return np.sum(self.train_labels == 1)\n",
    "\n",
    "    def get_train_dfs_count(self):\n",
    "        \"\"\"Returns the number of dfs samples in the training set.\"\"\"\n",
    "        return np.sum(self.train_labels == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Replace folder paths with where you have the images, demographics file, and annotation boxes file stored\n",
    "# You may need to make sure that the demographics csv has the first row as header/column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duke_list = pd.read_csv('/gs/gsfs0/users/rhadidchi/Duke pts.csv')\n",
    "duke_list['Patient ID'] = duke_list['Patient ID'].str[-3:]\n",
    "ids = list(duke_list['Patient ID'])\n",
    "pts = [f'/gs/gsfs0/users/rhadidchi/Duke/manifest-1725137251434/Duke-Breast-Cancer-MRI/Breast_MRI_{id}' for id in ids]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "duke_segmentations = pd.read_excel('/gs/gsfs0/users/rhadidchi/Annotation_Boxes.xlsx')\n",
    "duke, duke_pts = dcm_to_np_duke(pts, (4,64,64,64), device, duke_segmentations)  # (acquisitions,axial,coronal,sagittal)\n",
    "np.save('/gs/gsfs0/users/rhadidchi/duke4phases.npy', duke)\n",
    "duke.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T07:13:15.730656Z",
     "iopub.status.busy": "2025-01-23T07:13:15.729888Z",
     "iopub.status.idle": "2025-01-23T07:13:30.017655Z",
     "shell.execute_reply": "2025-01-23T07:13:30.016564Z",
     "shell.execute_reply.started": "2025-01-23T07:13:15.730625Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(922, 4, 64, 64, 64)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "duke = np.load('/gs/gsfs0/users/rhadidchi/duke4phases.npy') # '/kaggle/input/duke224/duke224.npy'\n",
    "duke.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T08:04:09.758152Z",
     "iopub.status.busy": "2025-01-23T08:04:09.757289Z",
     "iopub.status.idle": "2025-01-23T08:04:09.934013Z",
     "shell.execute_reply": "2025-01-23T08:04:09.933126Z",
     "shell.execute_reply.started": "2025-01-23T08:04:09.758116Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pts = pd.read_csv('/gs/gsfs0/users/rhadidchi/Duke pts.csv')\n",
    "pts['Patient ID'] = list(map(int, pts['Patient ID'].str[-3:]))\n",
    "pts['HR'] = pts['ER'] + pts['PR']\n",
    "pts = create_dummies(pts, ['Race and Ethnicity'])\n",
    "pts['Other Race'] = (pts['Race and Ethnicity_0'] | pts['Race and Ethnicity_3'] | pts['Race and Ethnicity_4'] | pts['Race and Ethnicity_5'] | pts['Race and Ethnicity_6'] | pts['Race and Ethnicity_7'] | pts['Race and Ethnicity_8']).astype(int)\n",
    "pts['T stage'] = pts['Staging(Tumor Size)# [T]']\n",
    "pts['N stage'] = pts['Staging(Nodes)#(Nx replaced by -1)[N]']\n",
    "pts['M stage'] = pts['Staging(Metastasis)#(Mx -replaced by -1)[M]']\n",
    "pts['Nodal Involvement'] = (pts['N stage'] > 0).astype(int)\n",
    "pts['High T Stage'] = (pts['T stage']>1).astype(int)\n",
    "pts['Nottingham N Score'] = pts['N stage'].apply(lambda x: 1 if x == 0 else 2 if x == 1 else 3)\n",
    "pts['Age'] = pts['Date of Birth (Days)']/(-365.25)\n",
    "pts = pts.dropna(subset=['Tumor Grade(T)','Tumor Grade(N)','Tumor Grade(M)'])\n",
    "pts['T Grade'] = pts['Tumor Grade(T)']\n",
    "pts['N Grade'] = pts['Tumor Grade(N)']\n",
    "pts['M Grade'] = pts['Tumor Grade(M)']\n",
    "pts = create_dummies(pts, ['T stage', 'N stage', 'M stage', 'T Grade', 'N Grade', 'M Grade'])\n",
    "pts['Grade'] = pts['T Grade'] + pts['N Grade'] + pts['M Grade']\n",
    "pts['Grade 7'] = (pts['Grade'] == 7).astype(int)\n",
    "pts['Nottingham Grade'] = pts['Grade'].apply(lambda x: 1 if x in [3,4,5] else 2 if x in [6,7] else 3)\n",
    "pts['Nottingham Grade 1'] = (pts['Nottingham Grade'] == 1).astype(int)\n",
    "pts['Nottingham Grade 2'] = (pts['Nottingham Grade'] == 2).astype(int)\n",
    "pts['Nottingham Grade 3'] = (pts['Nottingham Grade'] == 3).astype(int)\n",
    "pts['Nottingham Grade 2 or 3'] = (pts['Nottingham Grade'] > 1).astype(int)\n",
    "pts['Oncotype'] = pts['Oncotype score'].apply(lambda x: 0 if x < 18 else 1 if x >= 18 and x <= 30 else 2 if x > 30 else None)\n",
    "pts = create_dummies(pts, ['Oncotype'])\n",
    "pts['luminal A'] = ((pts['HR']>0)&(pts['HER2']==0)).astype(int) \n",
    "pts['luminal B'] = ((pts['HR']>0)&(pts['HER2']==1)).astype(int) \n",
    "pts['luminal'] = ((pts['luminal A'] == 1) | (pts['luminal B'] == 1)).astype(int)\n",
    "pts['Mol Subtype'] = pts.apply(lambda x: 5 if x['luminal B'] == 1 else x['Mol Subtype'], axis=1)\n",
    "pts['TN'] = (pts['Mol Subtype'] == 3).astype(int)\n",
    "pts['HRâHER2+'] = (pts['Mol Subtype'] == 2).astype(int)\n",
    "pts['HR+HER2+'] = (pts['Mol Subtype'] == 1).astype(int)\n",
    "pts['TN&HER2+'] = (pts['Mol Subtype'] > 1).astype(int)\n",
    "pts['Simplified Mol Subtype'] = pts['Mol Subtype'].apply(lambda x: 1 if x==0 else 2 if x in [1,2,5] else 3)\n",
    "pts['pCR'] = pts['Overall Near-complete Response:  Looser Definition'].apply(lambda x: 1 if x == 1 else 0)\n",
    "pts['HR+HER2â'] = ((pts['HR']>0) & (pts['HER2']==0)).astype(int)\n",
    "pts['HR+HER2+'] = ((pts['HR']>0) & (pts['HER2']==1)).astype(int)\n",
    "pts['Molecular Subtype'] = pts.apply(lambda x:\n",
    "    'ER+/HER2+(Â±PR)' if x['ER'] == 1 and x['HER2'] == 1 else\n",
    "    'ERâ/HER2+(Â±PR)' if x['ER'] == 0 and x['HER2'] == 1 else\n",
    "    'ER+/HER2â(Â±PR)' if x['ER'] == 1 and x['HER2'] == 0 else\n",
    "    'ERâ/HER2â(PRâ)' if x['ER'] == 0 and x['HER2'] == 0 else\n",
    "    'Unknown', axis=1)\n",
    "pts = create_dummies(pts, ['Molecular Subtype'], prefix=False)\n",
    "pts['Everyone'] = 'Everyone'\n",
    "pts['Ageâ¥50'] = (pts['Age']>=50).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T08:04:11.789527Z",
     "iopub.status.busy": "2025-01-23T08:04:11.789139Z",
     "iopub.status.idle": "2025-01-23T08:04:11.815715Z",
     "shell.execute_reply": "2025-01-23T08:04:11.814657Z",
     "shell.execute_reply.started": "2025-01-23T08:04:11.789494Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pts['RD'] = ((pts['Days to death (from the date of diagnosis) ']!='NP')|(pts['Recurrence event(s)']==1)|(pts['Days to local recurrence (from the date of diagnosis) ']!='NP')|(pts['Days to distant recurrence(from the date of diagnosis) ']!='NP')).astype(int)\n",
    "\n",
    "pts['Recurrence_TIME'] = np.where(\n",
    "    pts['Days to local recurrence (from the date of diagnosis) '] != 'NP',\n",
    "    pts['Days to local recurrence (from the date of diagnosis) '],\n",
    "    np.where(\n",
    "        pts['Days to distant recurrence(from the date of diagnosis) '] != 'NP',\n",
    "        pts['Days to distant recurrence(from the date of diagnosis) '],\n",
    "        'NP'\n",
    "    )\n",
    ")\n",
    "\n",
    "pts['Outcome_TIME'] = np.where(\n",
    "    (pts['Recurrence_TIME'] != 'NP') & (pts['Days to death (from the date of diagnosis) '] != 'NP'),\n",
    "    np.minimum(pts['Recurrence_TIME'], pts['Days to death (from the date of diagnosis) ']),\n",
    "    np.where(\n",
    "        pts['Recurrence_TIME'] != 'NP',\n",
    "        pts['Recurrence_TIME'],\n",
    "        np.where(\n",
    "            pts['Days to death (from the date of diagnosis) '] != 'NP',\n",
    "            pts['Days to death (from the date of diagnosis) '],\n",
    "            'NP'\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "pts['Days to last local recurrence free assessment (from the date of diagnosis) '] = pd.to_numeric(pts['Days to last local recurrence free assessment (from the date of diagnosis) '], errors='coerce')\n",
    "pts['Days to last distant recurrence free assemssment(from the date of diagnosis) '] = pd.to_numeric(pts['Days to last distant recurrence free assemssment(from the date of diagnosis) '], errors='coerce')\n",
    "\n",
    "pts['Non_Outcome_TIME'] = np.where(\n",
    "    pts['Days to last local recurrence free assessment (from the date of diagnosis) '].notna() & pts['Days to last distant recurrence free assemssment(from the date of diagnosis) '].notna() ,\n",
    "    np.maximum(pts['Days to last local recurrence free assessment (from the date of diagnosis) '], pts['Days to last distant recurrence free assemssment(from the date of diagnosis) ']),\n",
    "    np.where(\n",
    "        pts['Days to last local recurrence free assessment (from the date of diagnosis) '].notna(),\n",
    "        pts['Days to last local recurrence free assessment (from the date of diagnosis) '],\n",
    "        np.where(\n",
    "            pts['Days to last distant recurrence free assemssment(from the date of diagnosis) '].notna(),\n",
    "            pts['Days to last distant recurrence free assemssment(from the date of diagnosis) '],\n",
    "            'NA'\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "pts = pts[pts['Non_Outcome_TIME']!='NA']\n",
    "\n",
    "pts['Non_Outcome_TIME'] = pd.to_numeric(pts['Non_Outcome_TIME'], errors='coerce')\n",
    "pts['Outcome_TIME'] = pd.to_numeric(pts['Outcome_TIME'], errors='coerce')\n",
    "\n",
    "pts['RD_TIME'] = np.where(\n",
    "    pts['RD'] == 1,\n",
    "    pts['Outcome_TIME'],  # Take Outcome_TIME if RD == 1\n",
    "    pts['Non_Outcome_TIME']  # Take Non_Outcome_TIME if RD == 0\n",
    ")\n",
    "\n",
    "pts['RD_TIME'] = pts['RD_TIME'] / 365.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T07:14:11.777126Z",
     "iopub.status.busy": "2025-01-23T07:14:11.776557Z",
     "iopub.status.idle": "2025-01-23T07:14:11.839702Z",
     "shell.execute_reply": "2025-01-23T07:14:11.838895Z",
     "shell.execute_reply.started": "2025-01-23T07:14:11.777094Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "variable = 'Nottingham Grade'\n",
    "HRp = [id -1 for id in list(pts[pts[variable] == 1]['Patient ID'])]\n",
    "HRn = [id -1 for id in list(pts[pts[variable] == 3]['Patient ID'])]\n",
    "HRps = duke[HRp,:3,28:36,:,:]\n",
    "HRng = duke[HRn,:3,28:36,:,:]\n",
    "HRps.shape, HRng.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T07:14:14.937172Z",
     "iopub.status.busy": "2025-01-23T07:14:14.936849Z",
     "iopub.status.idle": "2025-01-23T07:14:14.998650Z",
     "shell.execute_reply": "2025-01-23T07:14:14.997712Z",
     "shell.execute_reply.started": "2025-01-23T07:14:14.937143Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MRIs = MRI(HRps, HRng, 42)\n",
    "MRIs.get_train_rd_count(), MRIs.get_train_dfs_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        self.cnn_model = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "        nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "        nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "        nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "        nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),  \n",
    "       \n",
    "        )\n",
    "       \n",
    "        self.fc_model = nn.Sequential(\n",
    "        nn.Linear(in_features=512, out_features=256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features=256, out_features=1)\n",
    "        )\n",
    "       \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_model(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_model(x)\n",
    "        x = F.sigmoid(x)\n",
    "       \n",
    "        return x\n",
    "\n",
    "set_seed(45)\n",
    "\n",
    "# Move model to device\n",
    "model = CNN().to(device)\n",
    "\n",
    "#model = CNN().to(device)\n",
    "train_model(model, MRIs, device, lrng_rt=5e-6, EPOCH=240, batch_size=32, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T08:04:21.360886Z",
     "iopub.status.busy": "2025-01-23T08:04:21.359983Z",
     "iopub.status.idle": "2025-01-23T08:04:21.470940Z",
     "shell.execute_reply": "2025-01-23T08:04:21.469855Z",
     "shell.execute_reply.started": "2025-01-23T08:04:21.360829Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pts['Probability of Grade 3'] = model(torch.tensor(duke[(pts['Patient ID']-1),:3,31], dtype=torch.float32).to(device)).squeeze().tolist()\n",
    "pts['Grade 2 High'] = (pts['Probability of Grade 3'] > 0.50).astype(int)\n",
    "pts[pts['Nottingham Grade']==2]['Grade 2 High'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts.to_csv('Duke RFS Model Results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T07:07:00.447673Z",
     "iopub.status.busy": "2025-01-23T07:07:00.447315Z",
     "iopub.status.idle": "2025-01-23T07:07:00.457272Z",
     "shell.execute_reply": "2025-01-23T07:07:00.456201Z",
     "shell.execute_reply.started": "2025-01-23T07:07:00.447644Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "population = pts[(pts['Nottingham Grade']==2)].reset_index(drop=True)\n",
    "len(population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T06:53:40.221621Z",
     "iopub.status.busy": "2025-01-23T06:53:40.221233Z",
     "iopub.status.idle": "2025-01-23T06:53:40.337663Z",
     "shell.execute_reply": "2025-01-23T06:53:40.336560Z",
     "shell.execute_reply.started": "2025-01-23T06:53:40.221576Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "outcomes = {\n",
    "    'RD': False\n",
    "}\n",
    "\n",
    "predictors = ['Grade 2 High', 'Age', 'Nodal Involvement', 'High T Stage', 'ER+/HER2+(Â±PR)', 'ERâ/HER2+(Â±PR)', 'ERâ/HER2â(PRâ)']\n",
    "\n",
    "full = fit_cox_model(population, outcomes, predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T06:54:08.759988Z",
     "iopub.status.busy": "2025-01-23T06:54:08.759099Z",
     "iopub.status.idle": "2025-01-23T06:54:08.889179Z",
     "shell.execute_reply": "2025-01-23T06:54:08.887828Z",
     "shell.execute_reply.started": "2025-01-23T06:54:08.759944Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "outcomes = {\n",
    "    'RD': False\n",
    "}\n",
    "\n",
    "predictors = ['Age', 'Nodal Involvement', 'High T Stage', 'ER+/HER2+(Â±PR)', 'ERâ/HER2+(Â±PR)', 'ERâ/HER2â(PRâ)']\n",
    "\n",
    "reduced = fit_cox_model(population, outcomes, predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T06:53:27.318806Z",
     "iopub.status.busy": "2025-01-23T06:53:27.318475Z",
     "iopub.status.idle": "2025-01-23T06:53:27.395035Z",
     "shell.execute_reply": "2025-01-23T06:53:27.393688Z",
     "shell.execute_reply.started": "2025-01-23T06:53:27.318778Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "comparison_result = bootstrap_compare_c_indices(\n",
    "    full['RD'],\n",
    "    reduced['RD'],\n",
    "    df=population, flip_sign=False\n",
    ")\n",
    "\n",
    "print(comparison_result)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5878179,
     "sourceId": 9629167,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5878226,
     "sourceId": 9629224,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5894710,
     "sourceId": 9650787,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5936323,
     "sourceId": 9706295,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5950098,
     "sourceId": 9724358,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6483703,
     "sourceId": 10471421,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6483749,
     "sourceId": 10471497,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
